\chapter[Background and Related Works]{Background}

\section{Background Materials}
\subsection{Ahead-of-time (AOT) Compilation}
In the Java Virtual Machine (JVM), byte code compilation can occur either at build time or runtime. Ahead-of-Time (AOT) compilation involves translating the byte code into machine code before execution, resulting in a fully compiled binary that is immediately executable \cite{Wade2017}. In contrast, Just-In-Time (JIT) Compilation defers the translation until runtime if at all, dynamically converting Java bytecodes into machine code within the JVM and optimizing frequently executed code paths to enhance performance \cite{Wade2017}. AOT compilation generally requires longer build times but offers rapid startup and predictable performance, making it suitable for applications where quick initialization is critical. JIT compilation, while benefiting from shorter build times, incurs longer startup periods but allows for more complex optimizations based on runtime profiling data.

JIT compilation enables techniques such as speculative optimizations, which make assumptions about a program’s behavior to apply performance-enhancing transformations based on runtime profiling. Although these optimizations can significantly improve performance by optimizing frequently executed code paths, incorrect assumptions may necessitate deoptimizations, which revert the code to a less optimized but more reliable version \cite{Duboscq2013Inproceedings}. This can complicate the program's IR by adding additional nodes and edges to accommodate these transformations \cite{Duboscq2013Inproceedings}. In this project, the emphasis will be on AOT compilation, where speculative optimizations are excluded \cite{Wimmer2019}, resulting in a simpler IR without the need for deoptimization.

\subsection{GraalVM Compiler Optimizations}

In GraalVM, the compilation process is divided into two main phases. The first phase, involving the Graal IR, handles most of the high-level optimizations. This phase is further organized into three tiers: high-tier for high-level optimizations, mid-tier for memory-focused enhancements, and low-tier for low-level IR (LIR) conversion \cite{Graal2021}. This project will primarily concentrate on high-tier optimizations within the GraalVM, beginning with the canonicalization phase.

Canonicalization, an essential early phase in the optimization process, focuses on transforming code into a standardized format and eliminating redundancies. This transformation simplifies and facilitates the application of subsequent optimizations. Examples of canonicalization include:

\begin{description}
    \item \textbf{Constant Folding:} Replace constant expressions with their computed values, such as simplifying \newline
    \(3 + 4\) to \(7\).
    \newpage
    \item \textbf{Simplify Redundant Multiplication:} Eliminate unnecessary multiplication operations, such as converting \(x * 0\) to 0, and \(x * 1\) to \(x\).
    \item \textbf{Simplifying Conditional Statements:} Reduce complexity in conditional logic by removing redundant or always false branches. For example, an if statement with a condition that can never be true, such as \(if (false)\), can be simplified by removing the entire branch.
    \item \textbf{Global Value Numbering:} Eliminate redundant computations by assigning unique identifiers to equivalent expressions \cite{Cliff1995}. For instance, if the expression \(a + b\) appears multiple times in the code, global value numbering ensures that it is computed only once and reused, thereby reducing unnecessary recalculations.
\end{description}

\subsection{Graal Intermediate Representation (IR)}

The Graal IR \cite{Duboscq2013} models a program's structure and operations using a directed graph that illustrates both data and control flow between nodes. Each node in this graph is designed to produce a single value and follows the Static Single Assignment (SSA) \cite{Ron1991} form. Data flow is represented by input edges pointing upward to the operand nodes, control flow is depicted by successor edges pointing downwards to the successor nodes. This IR framework provides a robust and efficient structure for code analysis and transformation, where optimization processes involve modifying the graph to enhance overall performance. A critical aspect of this project involves converting the Graal IR into Prolog facts to enable the optimizer to query these facts for potential optimizations. Therefore, it is essential to thoroughly understand the IR's structure and components to translate and use it within the Prolog-based optimization framework effectively.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{Packages/graphir.png}
    \caption{Simple IR Graph}
    \label{figure:graphir}
\end{figure}

\autoref{figure:graphir} illustrates a simple IR graph, with control flow edges highlighted in red. The graph begins at the Start node, which connects to the Return node via a successor edge. Upon reaching the Return node, the graph traverses upward through data flow edges to compute the returned expression’s value. This traversal demonstrates how control and data flows interact to process and complete the function's execution.

\subsection{Prolog Programming Language}

In traditional imperative languages, a program consists of a sequence of instructions. This approach emphasizes a step-by-step procedure where each instruction modifies the state of the machine to solve a given problem. In contrast, logic programming languages, such as Prolog, operate on a fundamentally different paradigm. Instead of prescribing a sequence of operations, logic programming focuses on defining a knowledge base composed of facts and rules \cite{Bramer2013}. After that, users can query the knowledge base to search for objects and relations. 

In Prolog, facts represent objects and their relationships, while rules imply the relationship between objects given it satisfies all the conditions. Once the knowledge base is established, users can formulate queries to extract information or solve problems by leveraging the logical relationships defined in the base using the depth-first search algorithm \cite{Chowdhary2020}. There may be several ways to achieve a given goal. The system initially selects the first available option. If Prolog fails to resolve a specific subgoal, it will backtrack to explore these previously noted alternatives. This mechanism, referred to as backtracking \cite{Chowdhary2020}, enables Prolog to systematically search for different solutions by revisiting and trying alternative paths.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Packages/Prolog.png}
    \caption{Example of Prolog specification}
    \label{figure:prolog}
\end{figure}

\autoref{figure:prolog} illustrates a Prolog program. The first three clauses are facts: john is a parent of tom and jessica, and jessica is a parent of both mary and andrew. The final clause is a rule defining the conditions for a grandparent relationship. When a query is made to determine the nodes for which john is a grandparent, Prolog performs a search from the first to the last clause, showcasing its backtracking behavior. Initially, Prolog identifies that john is a parent of tom, but since tom is not a parent of anyone, the search backtracks. It then considers the next option: jessica. Prolog then finds that jessica is a parent of mary. After exploring this path, Prolog backtracks again and continues to check the next possibility: jessica is also a parent of andrew, thereby completing the query.

\newpage
\section{Literature Review}

\subsection{Advancements Through Domain-Specific Languages (DSLs)}
Declarative domain-specific languages (DSLs) have made a big impact on compiler optimization by offering targeted solutions for specific optimization tasks. For example, Halide \cite{Jonathan2018} is a domain-specific language and compiler framework designed to optimize image processing and computational photography. Writing efficient code for image processing often requires complex optimizations to exploit parallelism and memory locality. In Halide, the algorithm specifies the computational logic for these tasks, detailing what needs to be done, such as resizing, sharpening, or blurring an image. The schedule defines the execution strategy, including how computations are ordered, parallelized, and managed in memory. This separation of concerns allows Halide to generate highly optimized code by focusing on both the functional description and the efficient execution of tasks. Meanwhile, Spinellis created a peephole optimizer that uses a declarative DSL to specify optimizations \cite{Spinellis1999}. Spinellis transformed specifications into string regular expressions, which are then applied to the target code, allowing for adaptive and efficient refinements. Although this method has only been tested on smaller programs and specific types of optimizations so far, it shows great potential for quickly experimenting with new techniques and architectures. Elevate \cite{Hagedorn2020} is another functional language designed to let programmers define custom optimization strategies. It allows for creating composable optimizations rather than sticking to predefined APIs. Its success in case studies and practical applications highlights its ability to handle complex optimization tasks and deliver strong performance.

\subsection{Advancements Through Logic Programming Languages}
DataLog has established itself as a powerful tool in compiler implementation through its applications in complex program analysis and transformation. For instance, Lam et al. developed a framework using Datalog queries and a specialized language called PQL, which employs deductive databases and binary decision diagrams (BDDs) to tackle complex issues like pointer aliasing and heap object management in a context-sensitive manner \cite{Lam2005}. This approach has simplified the creation of sophisticated analyses and has been crucial in identifying security vulnerabilities in C and Java applications. Similarly, the Soufflé Datalog engine \cite{silverman2021wantanalyzeschemeprograms} introduces a new method for control-flow analysis in Scheme, making it possible to perform scalable and complex analyses of functional programming constructs, and extending the benefits of Datalog-based static analysis to Scheme-like languages. Furthermore, DIMPLE \cite{Benton2007}, a framework for Java bytecode static analyses, is implemented in the Yap Prolog system. DIMPLE leverages logic programming capabilities to provide a declarative language for specifying analyses and representing Java bytecodes. The framework facilitates iterative experimentation and delivers efficient implementations that are on par with specialized tools. Finally, the Doop framework \cite{Bravenboer2009} utilizes Datalog to offer a declarative solution for points-to analysis in Java, providing impressive performance gains and scalability for precise context-sensitive analyses that were previously challenging to achieve. 
\newpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{Packages/Doop.png}
    \caption{DOOP's rule for virtual method invocations \cite{Bravenboer2009}}
    \label{figure:doop}
\end{figure}
\autoref{figure:doop} illustrates a Datadog rule that Doop uses to represent a call graph. The rule holds if there is a virtual method call where the receiver object is accessed through (?base) that points to (?heap) which contains (?callee) method in its type (?heaptype) \cite{Bravenboer2009}.

\subsection{Transformation, Rewriting Techniques, and Verification}
Transformation and rewriting techniques are essential to modern compiler optimization, enabling sophisticated processes like grammar specification, pattern matching, and program rewriting. Stratego \cite{Eelco2001} is a language designed for specifying program transformation systems using rewriting strategies, which allow for precise control over rule application. The compiler transforms these rules into C code and leverages the ATerm library for effective term representation. It has been used in several applications, including program transformation tools like XT, and domain-specific optimization frameworks like CodeBoost ~\cite{Eelco2001}. On the other hand, Veriopt \cite{Webb2023} offers a comprehensive framework for formal verification of Graal compiler optimizations. This framework employs  ``term rewriting rules on the abstract term representation" \cite{Webb2023} and then maps these rules to term graph transformations. In the Veriopt project, a term rewriting system, also known as an optimization phase, comprises a set of rules that can be applied repeatedly to optimize expressions. Each phase is associated with different types of root nodes in patterns, such as AddNode, AbsNode, and MulNode. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Packages/veriopt.png}
    \caption{Conditional canonicalization rules \cite{Webb2023}}
    \label{figure:veriopt}
\end{figure}

\newpage
\autoref{figure:veriopt} provides an example of such an optimization phase tailored for conditional expressions. NegateCond transforms negated conditions using logical equivalences, while TrueCond simplifies expressions where the condition is always true by directly applying the true outcome. Conversely, FalseCond addresses conditions that are always false by eliminating irrelevant branches. BranchEqual simplifies expressions with equality checks by optimizing or removing redundant comparisons.

\subsection{Identified Research Gaps}
Despite the established effectiveness of logic programming languages like Datalog in program analysis, there is limited research on integrating these techniques within modern compiler frameworks such as GraalVM. While significant advancements have been made in transformation and rewriting techniques using domain-specific languages (DSLs) and other paradigms, there is a notable absence of comparative studies on incorporating logic programming-based optimizations into GraalVM. This gap extends to a lack of empirical evidence assessing the impact of such techniques on optimization time and efficiency. Integrating Prolog-based rules with GraalVM’s graph-based IR is particularly challenging due to insufficient research on mapping Graal IR to Prolog and ensuring that Prolog-based optimizers work effectively within the Java environment. Addressing these challenges could enhance our understanding of how to apply logic programming in modern compilers and improve compiler optimization practices by providing valuable insights into the performance and effectiveness of these techniques.
